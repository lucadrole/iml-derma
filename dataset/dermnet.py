"""
DermNet episodic dataset for N-way, k-shot image classification with INPs.

Used as AUXILIARY meta-training data alongside ISIC 2019 to increase task
diversity. Only the 19 classes that do NOT overlap with ISIC 2019 are used.

Two-step workflow
-----------------
Step 1 – Pre-compute and cache CLIP embeddings (run once):

    python dataset/dermnet.py \
        --data-root data/dermnet \
        --encoder clip

    Produces:
        data/dermnet/embeddings_clip.pt

Step 2 – Use DermNetEpisodicDataset in training via mixed_episodic.py

Data layout (Kaggle download: shubhamgoel27/dermnet)
-----------------------------------------------------
    data/dermnet/
    ├── train/
    │   ├── Acne and Rosacea Photos/
    │   ├── Atopic Dermatitis Photos/
    │   └── ...  (23 subfolders)
    ├── test/
    │   └── ...  (same 23 subfolders)
    └── descriptions.json          ← generated by LLM (see prompt)

Overlap exclusion
-----------------
    4 DermNet classes overlap with ISIC 2019 and are EXCLUDED:
      - "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions"
      - "Melanoma Skin Cancer Nevi and Moles"
      - "Seborrheic Keratoses and other Benign Tumors"
      - "Vascular Tumors"
    This leaves 19 unique classes for auxiliary meta-training.
"""

import os
import json
import argparse
import glob

import numpy as np
import torch
from torch.utils.data import Dataset
from PIL import Image


# ---------------------------------------------------------------------------
# Class definitions
# ---------------------------------------------------------------------------

# The 4 DermNet classes that overlap with ISIC 2019
EXCLUDED_CLASSES = {
    "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions",
    "Melanoma Skin Cancer Nevi and Moles",
    "Seborrheic Keratoses and other Benign Tumors",
    "Vascular Tumors",
}

# All 23 DermNet top-level classes (Kaggle folder names)
ALL_23_CLASSES = [
    "Acne and Rosacea Photos",
    "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions",
    "Atopic Dermatitis Photos",
    "Bullous Disease Photos",
    "Cellulitis Impetigo and other Bacterial Infections",
    "Eczema Photos",
    "Exanthems and Drug Eruptions",
    "Hair Loss Photos Alopecia and other Hair Diseases",
    "Herpes HPV and other STDs Photos",
    "Light Diseases and Disorders of Pigmentation",
    "Lupus and other Connective Tissue diseases",
    "Melanoma Skin Cancer Nevi and Moles",
    "Nail Fungus and other Nail Disease",
    "Poison Ivy Photos and other Contact Dermatitis",
    "Psoriasis pictures Lichen Planus and related diseases",
    "Scabies Lyme Disease and other Infestations and Bites",
    "Seborrheic Keratoses and other Benign Tumors",
    "Systemic Disease",
    "Tinea Ringworm Candidiasis and other Fungal Infections",
    "Urticaria Hives",
    "Vascular Tumors",
    "Vasculitis Photos",
    "Warts Molluscum and other Viral Infections",
]

# The 19 classes we actually use (sorted for determinism)
USED_CLASSES = sorted([c for c in ALL_23_CLASSES if c not in EXCLUDED_CLASSES])

# 4 held-out classes for validation (visually diverse, sufficient images)
DERMNET_VAL_CLASSES = {
    "Psoriasis pictures Lichen Planus and related diseases",  # inflammatory/scaly
    "Bullous Disease Photos",                                  # blistering
    "Nail Fungus and other Nail Disease",                      # nail-specific
    "Urticaria Hives",                                         # raised wheals/allergic
}

# Meta-learning split: 15 train, 4 val
SPLIT_CLASSES = {
    "train": sorted([c for c in USED_CLASSES if c not in DERMNET_VAL_CLASSES]),
    "val":   sorted(DERMNET_VAL_CLASSES),
}


# ---------------------------------------------------------------------------
# Step 1 -- Embedding pre-computation
# ---------------------------------------------------------------------------

def build_embedding_cache(
    data_root: str,
    encoder: str = "clip",
    biomedclip_dir: str = None,
    batch_size: int = 128,
    min_images_per_class: int = 20,
) -> None:
    """
    Encode all DermNet images (train + test splits merged) with a frozen
    vision encoder and save a .pt cache file.

    Only the 19 non-overlapping classes are included.

    Cache schema
    ------------
    {
        "embeddings": Tensor[N, embed_dim],   float32, L2-normalised
        "labels":     Tensor[N],              int64, index into USED_CLASSES
        "class_names": list[str],             USED_CLASSES
        "encoder":    str,
        "embed_dim":  int,
    }
    """
    # Re-use the same encoder loading logic from isic.py
    import sys
    dataset_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, dataset_dir)
    from isic import _load_vision_encoder, _encode_images

    # ---- Collect image paths per class ----
    class_images = {}  # class_name -> list of image paths
    for split_dir in ["train", "test"]:
        base = os.path.join(data_root, split_dir)
        if not os.path.isdir(base):
            print(f"  Warning: {base} not found, skipping")
            continue
        for class_name in USED_CLASSES:
            folder = os.path.join(base, class_name)
            if not os.path.isdir(folder):
                continue
            imgs = glob.glob(os.path.join(folder, "*.jpg")) + \
                   glob.glob(os.path.join(folder, "*.jpeg")) + \
                   glob.glob(os.path.join(folder, "*.png"))
            if class_name not in class_images:
                class_images[class_name] = []
            class_images[class_name].extend(imgs)

    # Filter classes with too few images
    print(f"\nDermNet classes found: {len(class_images)}")
    for name in USED_CLASSES:
        n = len(class_images.get(name, []))
        keep = "✓" if n >= min_images_per_class else "✗ SKIP"
        print(f"  {name:60s}  {n:5d} images  {keep}")

    used = {
        name: paths for name, paths in class_images.items()
        if len(paths) >= min_images_per_class
    }
    print(f"\nClasses with >= {min_images_per_class} images: {len(used)}")

    # Build flat lists
    all_paths = []
    all_labels = []
    class_to_idx = {name: i for i, name in enumerate(USED_CLASSES) if name in used}

    for name in USED_CLASSES:
        if name not in used:
            continue
        idx = class_to_idx[name]
        for p in used[name]:
            all_paths.append(p)
            all_labels.append(idx)

    labels_tensor = torch.tensor(all_labels, dtype=torch.long)
    print(f"Total images to encode: {len(all_paths)}")

    # ---- Load encoder ----
    model, preprocess = _load_vision_encoder(encoder, biomedclip_dir)
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Encoding with {encoder!r} on {device}")

    # ---- Encode in batches ----
    all_embeds = []
    n = len(all_paths)

    for start in range(0, n, batch_size):
        batch_paths = all_paths[start: start + batch_size]
        imgs = []
        for path in batch_paths:
            try:
                img = Image.open(path).convert("RGB")
                imgs.append(preprocess(img))
            except Exception as e:
                print(f"  Warning: failed to load {path}: {e}")
                # Use a black image as placeholder
                imgs.append(torch.zeros(3, 224, 224))

        imgs_t = torch.stack(imgs).to(device)
        with torch.no_grad():
            embeds = _encode_images(model, imgs_t)
        all_embeds.append(embeds.cpu())

        done = start + len(batch_paths)
        if done % 2000 < batch_size or done == n:
            print(f"  {done}/{n}")

    embeddings = torch.cat(all_embeds, dim=0)
    embed_dim = int(embeddings.shape[1])

    # ---- Save ----
    out_path = os.path.join(data_root, f"embeddings_{encoder}.pt")
    torch.save({
        "embeddings":  embeddings,
        "labels":      labels_tensor,
        "class_names": USED_CLASSES,
        "class_to_idx": class_to_idx,
        "encoder":     encoder,
        "embed_dim":   embed_dim,
    }, out_path)

    print(f"\nSaved -> {out_path}")
    print(f"  Shape    : {list(embeddings.shape)}")
    print(f"  embed_dim: {embed_dim}")
    for name in USED_CLASSES:
        if name in class_to_idx:
            idx = class_to_idx[name]
            count = int((labels_tensor == idx).sum())
            print(f"  {idx:2d}  {name:60s}  {count:5d} images")


# ---------------------------------------------------------------------------
# Step 2 -- Episodic dataset
# ---------------------------------------------------------------------------

class DermNetEpisodicDataset(Dataset):
    """
    N-way, k-shot episodic dataset over cached DermNet embeddings.

    Returns the SAME tuple format as ISICEpisodicDataset:
        (x_context, y_context, x_query, y_query, knowledge, task_id)

    Split:
        "train" -> 15 classes (used for auxiliary meta-training)
        "val"   -> 4 held-out classes (used for mixed validation)
    """

    def __init__(
        self,
        split: str = "train",
        data_root: str = "data/dermnet",
        encoder: str = "clip",
        n_ways: int = 2,
        min_shots: int = 0,
        max_shots: int = 10,
        num_query: int = 20,
        n_episodes: int = 10_000,
        use_knowledge: bool = True,
        seed: int = None,
    ):
        assert split in ("train", "val"), f"Bad split: {split!r}"

        self.split         = split
        self.min_shots     = min_shots
        self.max_shots     = max_shots
        self.num_query     = num_query
        self.n_episodes    = n_episodes
        self.n_ways        = n_ways
        self.use_knowledge = use_knowledge
        self.seed          = seed

        # ---- Load embedding cache ----
        cache_path = os.path.join(data_root, f"embeddings_{encoder}.pt")
        if not os.path.exists(cache_path):
            raise FileNotFoundError(
                f"Cache not found: {cache_path}\n"
                f"Run first:  python dataset/dermnet.py "
                f"--data-root {data_root} --encoder {encoder}"
            )
        cache = torch.load(cache_path, map_location="cpu")
        all_embeddings = cache["embeddings"]
        all_labels     = cache["labels"]
        self.embed_dim = int(cache["embed_dim"])
        class_to_idx   = cache["class_to_idx"]

        # ---- Per-class embedding pools ----
        # Only keep classes that belong to this split AND are in the cache
        split_class_names = SPLIT_CLASSES[split]
        self.class_names = [name for name in split_class_names if name in class_to_idx]
        self.class_embeddings = {}

        for local_idx, name in enumerate(self.class_names):
            global_idx = class_to_idx[name]
            mask = (all_labels == global_idx)
            self.class_embeddings[local_idx] = all_embeddings[mask]

        n_avail = len(self.class_names)
        assert n_ways <= n_avail, (
            f"n_ways={n_ways} > available DermNet classes ({n_avail})"
        )

        # ---- Knowledge descriptions ----
        self.descriptions = None
        if use_knowledge:
            desc_path = os.path.join(data_root, "descriptions.json")
            if not os.path.exists(desc_path):
                raise FileNotFoundError(
                    f"Descriptions not found: {desc_path}\n"
                    "Generate them with the provided prompt."
                )
            with open(desc_path) as f:
                desc_dict = json.load(f)

            missing = [n for n in self.class_names if n not in desc_dict]
            if missing:
                raise ValueError(f"Missing DermNet descriptions for: {missing}")

            self.descriptions = {
                local_idx: desc_dict[name]
                for local_idx, name in enumerate(self.class_names)
            }

        # ---- Summary ----
        print(f"DermNetEpisodicDataset [{split}]  n_ways={self.n_ways}  "
              f"shots=[{min_shots},{max_shots}]  query/class={num_query}  "
              f"episodes={n_episodes}  classes={n_avail}")
        for li, name in enumerate(self.class_names):
            n = len(self.class_embeddings[li])
            print(f"  [{li:2d}] {name:60s}  {n} images")

    def __len__(self):
        return self.n_episodes

    def __getitem__(self, idx: int):
        # Per-episode RNG
        rng = np.random.default_rng(
            self.seed * 100_000 + idx if self.seed is not None else None
        )

        # Sample N classes
        chosen_local = sorted(
            rng.choice(
                len(self.class_names), self.n_ways, replace=False
            ).tolist()
        )
        ep_label = {cls: ep for ep, cls in enumerate(chosen_local)}

        # Sample k shots
        k = int(rng.integers(self.min_shots, self.max_shots + 1))

        x_ctx_parts, y_ctx_parts = [], []
        x_qry_parts, y_qry_parts = [], []

        for cls in chosen_local:
            ep_lbl = ep_label[cls]
            pool   = self.class_embeddings[cls]
            n_img  = len(pool)
            need   = k + self.num_query

            if need > n_img:
                indices = rng.choice(n_img, need, replace=True)
            else:
                indices = rng.choice(n_img, need, replace=False)

            if k > 0:
                x_ctx_parts.append(pool[indices[:k]])
                y_ctx_parts.append(
                    torch.full((k,), ep_lbl, dtype=torch.long)
                )

            x_qry_parts.append(pool[indices[k: k + self.num_query]])
            y_qry_parts.append(
                torch.full((self.num_query,), ep_lbl, dtype=torch.long)
            )

        if k > 0:
            x_context = torch.cat(x_ctx_parts, dim=0)
            y_context = torch.cat(y_ctx_parts, dim=0)
        else:
            x_context = torch.zeros(0, self.embed_dim)
            y_context = torch.zeros(0, dtype=torch.long)

        x_query = torch.cat(x_qry_parts, dim=0)
        y_query = torch.cat(y_qry_parts, dim=0)

        if self.use_knowledge and self.descriptions is not None:
            knowledge = [self.descriptions[cls] for cls in chosen_local]
        else:
            knowledge = None

        return x_context, y_context, x_query, y_query, knowledge, idx


# ---------------------------------------------------------------------------
# CLI -- Step 1
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Pre-compute CLIP embeddings for DermNet dataset."
    )
    parser.add_argument(
        "--data-root", type=str, default="data/dermnet",
        help="Folder with train/ and test/ subfolders",
    )
    parser.add_argument(
        "--encoder", type=str, default="clip",
        choices=["clip", "biomedclip"],
    )
    parser.add_argument(
        "--biomedclip-dir", type=str, default=None,
    )
    parser.add_argument(
        "--batch-size", type=int, default=128,
    )
    parser.add_argument(
        "--min-images", type=int, default=20,
        help="Skip classes with fewer images than this",
    )
    args = parser.parse_args()

    build_embedding_cache(
        data_root=args.data_root,
        encoder=args.encoder,
        biomedclip_dir=args.biomedclip_dir,
        batch_size=args.batch_size,
        min_images_per_class=args.min_images,
    )












































